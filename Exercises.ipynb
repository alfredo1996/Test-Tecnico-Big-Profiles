{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercises.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz6icjK3LsbRYLE7cH3l3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredo1996/Test-Tecnico-Big-Profiles/blob/main/Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MQPDGB6xNW-"
      },
      "source": [
        "#Spark installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoAmtcT8BXG9"
      },
      "source": [
        "This code is here because is necessary for the installation on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NIxnjLpw7iF"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKxELUbb_lPD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "ee737f44-b4be-46cf-806b-1545ce832604"
      },
      "source": [
        "import time\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a3207f30de94:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f90aa26e210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6whudkk9xT8q"
      },
      "source": [
        "#Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-1I3i1eHsPW"
      },
      "source": [
        "Execution time on Colab: approximately 1 minute\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PLIvJfPO46P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45728e0-4b6f-48c0-a358-c25743f198db"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id, expr\n",
        "from pyspark.sql import Window\n",
        "\n",
        "#Read file using spark.read_txt\n",
        "data = spark.read.text(\"/content/sample_data/exercise_1.txt\")\n",
        "data = data.withColumn(\"value\", data[\"value\"].cast(IntegerType()))\n",
        "\n",
        "\"\"\"\n",
        "Generator function for the aggregation of each value inside of the partition\n",
        "\"\"\"\n",
        "def sumInsidePartition(partition):\n",
        "  i = 0\n",
        "  for value in partition:\n",
        "    i = value[0]+i\n",
        "  yield i\n",
        "\n",
        "#Call the generator function on each repartition to find the sum\n",
        "df = data.rdd.repartition(100).mapPartitions(sumInsidePartition).toDF(\"string\")\n",
        "\n",
        "#Generate the index column for the output, useful also for the cumulative sum\n",
        "df = df.withColumn(\n",
        "    \"index\",\n",
        "    row_number().over(Window.orderBy(monotonically_increasing_id()))\n",
        ")\n",
        "\n",
        "#Generate the cumulative sum ordered by the index\n",
        "df = df.withColumn('Cumulative Sum', \n",
        "    expr('sum(value) over (order by index)'))\n",
        "\n",
        "#Drop the value column because it's unnecessary\n",
        "df = df.drop('value').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------+\n",
            "|index|Cumulative Sum|\n",
            "+-----+--------------+\n",
            "|    1|      -48479.0|\n",
            "|    2|      -93898.0|\n",
            "|    3|     -141829.0|\n",
            "|    4|     -192245.0|\n",
            "|    5|     -241620.0|\n",
            "|    6|     -290349.0|\n",
            "|    7|     -340332.0|\n",
            "|    8|     -388562.0|\n",
            "|    9|     -440468.0|\n",
            "|   10|     -488113.0|\n",
            "|   11|     -540788.0|\n",
            "|   12|     -591341.0|\n",
            "|   13|     -646322.0|\n",
            "|   14|     -696615.0|\n",
            "|   15|     -746863.0|\n",
            "|   16|     -798172.0|\n",
            "|   17|     -851906.0|\n",
            "|   18|     -901855.0|\n",
            "|   19|     -955747.0|\n",
            "|   20|    -1005959.0|\n",
            "+-----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN8sGA1oH0x4"
      },
      "source": [
        "After the first solution, i defined a second solution to improve its performance, because the RDD repartition was too slow in the first solution. The improvement was approximately of 55%.\n",
        "\n",
        "Execution time on Colab: approximately 27 seconds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSbhyX8m-hUS",
        "outputId": "248e4b4f-ffec-49f4-8ae8-d78c21141336"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id, expr\n",
        "from pyspark.sql import Window\n",
        "\n",
        "#Read file using spark.read_txt\n",
        "data = spark.read.text(\"/content/sample_data/exercise_1.txt\")\n",
        "data = data.withColumn(\"value\", data[\"value\"].cast(IntegerType()))\n",
        "\n",
        "#Find the cardinality of each window\n",
        "window_cardinality = data.select(\"value\").count()/100\n",
        "\n",
        "#Define the window index\n",
        "df = data.withColumn(\n",
        "    \"index\",\n",
        "    F.ceil(row_number().over(Window.orderBy(monotonically_increasing_id()))/window_cardinality)\n",
        ")\n",
        "\n",
        "#Sum over the window index and define \n",
        "df = df.groupBy(\"index\").sum(\"value\").withColumnRenamed(\"sum(value)\",\"TempSum\")\n",
        "\n",
        "#Find the cumulative sum of each window\n",
        "df = df.withColumn('Cumulative Sum', \n",
        "    expr('sum(TempSum) over (order by index)'))\n",
        "\n",
        "#Drop the value column because it's unnecessary\n",
        "df = df.drop('TempSum').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------+\n",
            "|index|Cumulative Sum|\n",
            "+-----+--------------+\n",
            "|    1|        -50969|\n",
            "|    2|        -98419|\n",
            "|    3|       -147135|\n",
            "|    4|       -195280|\n",
            "|    5|       -244419|\n",
            "|    6|       -294926|\n",
            "|    7|       -344856|\n",
            "|    8|       -394347|\n",
            "|    9|       -441334|\n",
            "|   10|       -492367|\n",
            "|   11|       -543368|\n",
            "|   12|       -592760|\n",
            "|   13|       -641927|\n",
            "|   14|       -691543|\n",
            "|   15|       -739688|\n",
            "|   16|       -787765|\n",
            "|   17|       -836532|\n",
            "|   18|       -888248|\n",
            "|   19|       -939387|\n",
            "|   20|       -986280|\n",
            "+-----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNR5LPYP2J7m"
      },
      "source": [
        "As you can see, the results generated by the two jobs are different. This because the dataset's order is not conserved during the computation with RDDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOwnBhAVybc-"
      },
      "source": [
        "#Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUEkqO7pKP8Y"
      },
      "source": [
        "Execution time on Colab: approximately 25 seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o69XvKKtya38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f05680d-4b2d-443d-d79f-babc42d9c0a2"
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, IntegerType\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "#Read file using spark.read_txt\n",
        "data = spark.read.option(\"header\",True).csv(\"/content/sample_data/exercise_2.txt\")\n",
        "data = data.withColumn(\"Value\", data[\"Value\"].cast(IntegerType()))\n",
        "\n",
        "#Find the mean of the all the values\n",
        "total_mean = data.select(expr(\"avg(value) as TotalMean\")).head()[0]\n",
        "\n",
        "#Find the aggregated average for each group\n",
        "data_agg = data.groupBy(\"ID\").avg(\"Value\").orderBy(\"ID\").withColumnRenamed(\"avg(Value)\",\"Mean\")\n",
        "\n",
        "#For calculus purposes, we round the numbers to avoid underflow problems\n",
        "data_agg = data_agg.withColumn(\"Mean\",F.round(data_agg[\"Mean\"],4))\n",
        "\n",
        "#Find the distance and write them inside a new column\n",
        "data_agg = data_agg.withColumn(\"Distance\", F.round(data_agg[\"Mean\"] - total_mean,4)).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+--------+\n",
            "| ID|   Mean|Distance|\n",
            "+---+-------+--------+\n",
            "|  A|-0.4889|  0.0109|\n",
            "|  B|-0.5041| -0.0043|\n",
            "|  C|-0.4803|  0.0195|\n",
            "|  D| -0.507| -0.0072|\n",
            "|  E|-0.4988|   0.001|\n",
            "|  F|-0.5044| -0.0046|\n",
            "|  G|-0.4942|  0.0056|\n",
            "|  H|-0.4994|  4.0E-4|\n",
            "|  I|-0.5006| -8.0E-4|\n",
            "|  J|-0.4952|  0.0046|\n",
            "|  K|-0.5111| -0.0113|\n",
            "|  L| -0.516| -0.0162|\n",
            "|  M|-0.5061| -0.0063|\n",
            "|  N|-0.5098|   -0.01|\n",
            "|  O|-0.4912|  0.0086|\n",
            "|  P|-0.4853|  0.0145|\n",
            "|  Q|-0.5035| -0.0037|\n",
            "|  R|-0.4883|  0.0115|\n",
            "|  S|-0.4973|  0.0025|\n",
            "|  T|-0.5048|  -0.005|\n",
            "+---+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}